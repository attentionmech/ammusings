# 0112 - 12th January

[1]

*What are heads doing*

In Deep neural nets which are built on the paradigm of attention, there is a concept of multi-head attention. It is like having many slices of a dish being independently seen by different people, and the hope is that they will come up with a unique view about the dish. In practice, this is happening due to the backprop being a hammer which can optimise almost everything which is differentiable.
